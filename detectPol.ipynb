{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe-8s44lvYpe"
   },
   "outputs": [],
   "source": [
    "! pip install langid\n",
    "! pip install normalise\n",
    "! pip install spacy\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjrMXrBvLs5E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentina/anaconda3/envs/ai/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/valentina/anaconda3/envs/ai/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "#import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('names')\n",
    "#nltk.download('brown')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from normalise import normalise\n",
    "import string\n",
    "import io\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import langid\n",
    "import sys\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "# Neural network\n",
    "#!{sys.executable} -m spacy download en\n",
    "#from spacy_langdetect import LanguageDetector\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the subset of yelp dataset cleaned\n",
    "df2 = pd.read_csv('subset_review.csv', sep='\\t' )\n",
    "del df2[\"Unnamed: 0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UIMDv_VBBY9"
   },
   "source": [
    "Starting text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ahlsT4nGfLz"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#tokenization, stop word removal and punctuation removal using spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "df2['cleaned_text'] = df2['text'].apply(lambda x: [t.lemma_.lower() for t in nlp.tokenizer(x) if ((not t.is_stop) and ( not t.is_punct) and (not t.is_currency) and (not t.is_digit) and (not t.is_space) and (t.is_alpha)) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.to_csv('subset_review_spacy.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>These guys are awesome. I have been coming her...</td>\n",
       "      <td>en</td>\n",
       "      <td>['guy', 'awesome', 'come', 'year', 'restaurant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Stopped by on Tuesday to look for some Glock 2...</td>\n",
       "      <td>en</td>\n",
       "      <td>['stopped', 'tuesday', 'look', 'glock', 'mags'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Had a great experience here. Staff was really ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['great', 'experience', 'staff', 'nice', 'atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This place is a complete joke. My fiancé order...</td>\n",
       "      <td>en</td>\n",
       "      <td>['place', 'complete', 'joke', 'fiancé', 'order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Can you put more salt on you fries really my l...</td>\n",
       "      <td>en</td>\n",
       "      <td>['salt', 'fry', 'lip', 'burn', 'plz', 'okay', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99590</th>\n",
       "      <td>2</td>\n",
       "      <td>Came on BlogTO's recommendation. This restaura...</td>\n",
       "      <td>en</td>\n",
       "      <td>['came', 'blogto', 'recommendation', 'restaura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99591</th>\n",
       "      <td>4</td>\n",
       "      <td>Ordered delivery to my hotel. They beat their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['ordered', 'delivery', 'hotel', 'beat', 'esti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99592</th>\n",
       "      <td>5</td>\n",
       "      <td>Deserves all five stars! Got all my questions ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['deserves', 'star', 'got', 'question', 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99593</th>\n",
       "      <td>2</td>\n",
       "      <td>Let me cry for a second before I write this re...</td>\n",
       "      <td>en</td>\n",
       "      <td>['let', 'cry', '2', 'write', 'review', 'cry', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99594</th>\n",
       "      <td>1</td>\n",
       "      <td>Making this review two days out and still feel...</td>\n",
       "      <td>en</td>\n",
       "      <td>['making', 'review', 'day', 'feel', 'need', 'w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99595 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stars                                               text language  \\\n",
       "0          5  These guys are awesome. I have been coming her...       en   \n",
       "1          5  Stopped by on Tuesday to look for some Glock 2...       en   \n",
       "2          5  Had a great experience here. Staff was really ...       en   \n",
       "3          1  This place is a complete joke. My fiancé order...       en   \n",
       "4          1  Can you put more salt on you fries really my l...       en   \n",
       "...      ...                                                ...      ...   \n",
       "99590      2  Came on BlogTO's recommendation. This restaura...       en   \n",
       "99591      4  Ordered delivery to my hotel. They beat their ...       en   \n",
       "99592      5  Deserves all five stars! Got all my questions ...       en   \n",
       "99593      2  Let me cry for a second before I write this re...       en   \n",
       "99594      1  Making this review two days out and still feel...       en   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0      ['guy', 'awesome', 'come', 'year', 'restaurant...  \n",
       "1      ['stopped', 'tuesday', 'look', 'glock', 'mags'...  \n",
       "2      ['great', 'experience', 'staff', 'nice', 'atte...  \n",
       "3      ['place', 'complete', 'joke', 'fiancé', 'order...  \n",
       "4      ['salt', 'fry', 'lip', 'burn', 'plz', 'okay', ...  \n",
       "...                                                  ...  \n",
       "99590  ['came', 'blogto', 'recommendation', 'restaura...  \n",
       "99591  ['ordered', 'delivery', 'hotel', 'beat', 'esti...  \n",
       "99592  ['deserves', 'star', 'got', 'question', 'answe...  \n",
       "99593  ['let', 'cry', '2', 'write', 'review', 'cry', ...  \n",
       "99594  ['making', 'review', 'day', 'feel', 'need', 'w...  \n",
       "\n",
       "[99595 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('subset_review_spacy.csv', sep='\\t' )\n",
    "del df2[\"Unnamed: 0\"]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXlMkam5t0lt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#sentence = \"This restaurant is amazzzinggg! I'll come back for sure ewwwww\"\n",
    "sentence = \"chillin'\"\n",
    "\n",
    "#import re\n",
    "#s = re.sub(r'(.)\\1+', r'\\1', sentence) \n",
    "#print(s)\n",
    "nlt = [t.lower() for t in normalise(word_tokenize(sentence)) if ( (not t in stopwords.words(\"english\") ) & (t.isalpha())) ]\n",
    "print(\"nltk: \" ,nlt)\n",
    "#spac = [t.lemma_.lower() for t in nlp.tokenizer(sentence) if ((not t.is_stop) & ( not t.is_punct) & (not t.is_currency) & (not t.is_digit) & (not t.is_space) & (t.is_alpha)) ]\n",
    "#print(\"spacy: \", spac)\n",
    "\"\"\"\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57epH6S6NzC-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518\n",
      "447.4214267784527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5,\n",
       "        'Full Review in Blog.\\n\\nA BYOB located in a rather blue collar part of town the restaurant may seem an odd choice to the unfamiliar-everything about its location, size, and layout seems to say \"locals only.\" What those paying attention would realize, however, is that Chef Douglas Dick has twice been a Beard Award Semi-Finalist, thrice been named Pittsburgh Magazine\\'s Chef of the Year, and has been embracing the \"Farm to Table\" movement since before most had American\\'s even heard of it.  Locally sourced foods prepared by an internationally educated chef and a menu that changes daily (because Dick goes to the market daily for his products) everything about Bona Terra sounded fantastic.\\n\\nGiven our large lunch and long drive ahead it was decided that we would share a couple of small appetizer plates prior to our mains - and given the fact that neither appetizer required cooking they would arrive quite quickly after our amuse.  Starting first with a cheese plate, Cypress Grove Lamb Chopper with poached Bartlett Pears, Mixed Berry Infused Honey.  Brought forth by the same company who makes my favorite cheese of all time, Humboldt Fog, the Lamb Chopper was described as a sheep\\'s milk Gouda imported from Holland.  Smooth as butter and fragrant without being pungent the fromage was accompanied by impossibly sweet pears, water crackers, local strawberries, and a clover honey blended with what tasted largely of blackberry.  A very well composed plate and a great cheese.\\n\\nFollowing our appetizers would be a short delay-but a delay with the most unexpected treasure of the night...a bread and butter combo that still leaves me weak in the knees.  Served singly (thank goodness or I\\'d have invariably eaten ten,) and piping hot from a wooden basket were \"Brioche Rolls and Salted Peach Butter.\"  Warm and buttery on their own the rollswould have been suitable without any topping - but paired with a peachy sweet mélange of butter and cream cheese, they were heavenly. All told I ate four...in retrospect I still wish I would have eaten ten.\\n\\nFor my sister\\'s main - Oven Roasted North Atlantic Cod over Israeli Cous Cous and Sauteed Green Beans, Tarragon Infused Butter-Wine Pan Sauce with Grilled Black Mission Figs.  Having never seen fig meets fish on a single plate I was intrigued by this dish from the start and only moreso when it arrived.  A substantial slice of mild and buttery cod over a toothsome cous-cous and pan-crispy green beans, the fish was topped with a halved plump and juicy Fig clearly soaked in wine and butter while a sweet/savory reduction of butter, wine, and chopped figs circulated the plate.  While the plating, in my opinion, could have been dressed up I\\'m rather certain the flavoring and quality couldn\\'t have been improved.\\n\\nWhile my companions\\' dishes were good mine was unreal - and only improved from the menu listing when Chef Dick informed us that he\\'d just gotten in a lobe of foie gras and if it \"met his approval\" he\\'d like to add it to the dish.  Arriving with the smells of cinnamon, cherry, allspice, and thyme the \"chef suggests medium rare\" Seared Maple Leaf farms Duck breast and Hudson Valley Foie Gras over long grain Basmati rice, Sauteed Zucchini, Local Sour Cherry Compote, Sweet and Spicy Tropical Jamaican Jerk Reduction was fantastic.  Smooth and supple with a clean layer of fat ribboned beneath the crispy skin the duck was potentially the best quality duck meat I\\'ve ever had.  Topped with a velvet and ample slice of foie gras a presented over a benign rice and vegetal zucchini mélange the dish was brought to a peak by the addition of poached black cherries and a reduction that added heat and spice without overwhelming the multiple nuances of the dish.\\n\\nAfter a meal so stunning dessert was a must - For my sister\\'s selection she targeted the Bavarian Chocolate and Hazelnut Tart with Local Strawberries.  Served in a buttery crust similar to my mothers, Erika\\'s tart would consist of a creamy whipped dark chocolate mousse loaded with what I can only describe as \"Chunky Hazelnut Butter.\"  Something like a \"dark\" Nutella pie the simplisticy of the tart was matched only by its decadence.\\n\\nFor my dessert the decision was between cheesecake and the dish our server described as the \"house special.\" With everything exceptionally special so far I went with the obvious - a simple and decidedly indulgent Rich Chocolate Cake with Peach Ice Cream.  Served almost like Keller\\'s Bouchon dessert the thick, rich, and warm cake was topped with a large scoop of ice-cream that tasted like a melting peach without the fuzz.  Focusing on the cake-the texture was almost that of a steamed pudding-a little wet, but \"set\" and hefty.\\n\\nBona Terra was one of those experiences that makes you realize just how good largely unmanipulated fresh fruits, vegetables, and proteins can be when placed in the hands of a skilled chef.  Great food, wonderful service, and a nice location without all the hype and frills - a \"hidden gem.\"',\n",
       "        'en',\n",
       "        \"['review', 'blog', 'byob', 'locate', 'blue', 'collar', 'town', 'restaurant', 'odd', 'choice', 'unfamiliar', 'location', 'size', 'layout', 'local', 'pay', 'attention', 'realize', 'chef', 'douglas', 'dick', 'twice', 'beard', 'award', 'semi', 'finalist', 'thrice', 'name', 'pittsburgh', 'magazine', 'chef', 'year', 'embrace', 'farm', 'table', 'movement', 'american', 'hear', 'locally', 'source', 'food', 'prepare', 'internationally', 'educate', 'chef', 'menu', 'change', 'daily', 'dick', 'go', 'market', 'daily', 'product', 'bona', 'terra', 'sound', 'fantastic', 'given', 'large', 'lunch', 'long', 'drive', 'ahead', 'decide', 'share', 'couple', 'small', 'appetizer', 'plate', 'prior', 'main', 'give', 'fact', 'appetizer', 'require', 'cook', 'arrive', 'quickly', 'amuse', 'starting', 'cheese', 'plate', 'cypress', 'grove', 'lamb', 'chopper', 'poach', 'bartlett', 'pears', 'mixed', 'berry', 'infused', 'honey', 'brought', 'forth', 'company', 'make', 'favorite', 'cheese', 'time', 'humboldt', 'fog', 'lamb', 'chopper', 'describe', 'sheep', 'milk', 'gouda', 'import', 'holland', 'smooth', 'butter', 'fragrant', 'pungent', 'fromage', 'accompany', 'impossibly', 'sweet', 'pear', 'water', 'cracker', 'local', 'strawberry', 'clover', 'honey', 'blend', 'taste', 'largely', 'blackberry', 'compose', 'plate', 'great', 'cheese', 'following', 'appetizer', 'short', 'delay', 'delay', 'unexpected', 'treasure', 'night', 'bread', 'butter', 'combo', 'leave', 'weak', 'knee', 'served', 'singly', 'thank', 'goodness', 'invariably', 'eat', 'pipe', 'hot', 'wooden', 'basket', 'brioche', 'rolls', 'salted', 'peach', 'butter', 'warm', 'buttery', 'rollswould', 'suitable', 'top', 'pair', 'peachy', 'sweet', 'mélange', 'butter', 'cream', 'cheese', 'heavenly', 'tell', 'eat', 'retrospect', 'wish', 'eat', 'sister', 'main', 'oven', 'roasted', 'north', 'atlantic', 'cod', 'israeli', 'cous', 'cous', 'sauteed', 'green', 'beans', 'tarragon', 'infused', 'butter', 'wine', 'pan', 'sauce', 'grilled', 'black', 'mission', 'figs', 'having', 'see', 'fig', 'meet', 'fish', 'single', 'plate', 'intrigue', 'dish', 'start', 'moreso', 'arrive', 'substantial', 'slice', 'mild', 'buttery', 'cod', 'toothsome', 'cous', 'cous', 'pan', 'crispy', 'green', 'bean', 'fish', 'top', 'halve', 'plump', 'juicy', 'fig', 'clearly', 'soak', 'wine', 'butter', 'sweet', 'savory', 'reduction', 'butter', 'wine', 'chop', 'fig', 'circulate', 'plate', 'plate', 'opinion', 'dress', 'certain', 'flavor', 'quality', 'improve', 'companion', 'dish', 'good', 'unreal', 'improve', 'menu', 'list', 'chef', 'dick', 'inform', 'get', 'lobe', 'foie', 'gras', 'meet', 'approval', 'like', 'add', 'dish', 'arriving', 'smell', 'cinnamon', 'cherry', 'allspice', 'thyme', 'chef', 'suggest', 'medium', 'rare', 'seared', 'maple', 'leaf', 'farm', 'duck', 'breast', 'hudson', 'valley', 'foie', 'gras', 'long', 'grain', 'basmati', 'rice', 'sauteed', 'zucchini', 'local', 'sour', 'cherry', 'compote', 'sweet', 'spicy', 'tropical', 'jamaican', 'jerk', 'reduction', 'fantastic', 'smooth', 'supple', 'clean', 'layer', 'fat', 'ribboned', 'beneath', 'crispy', 'skin', 'duck', 'potentially', 'well', 'quality', 'duck', 'meat', 'topped', 'velvet', 'ample', 'slice', 'foie', 'gras', 'present', 'benign', 'rice', 'vegetal', 'zucchini', 'mélange', 'dish', 'bring', 'peak', 'addition', 'poach', 'black', 'cherry', 'reduction', 'add', 'heat', 'spice', 'overwhelm', 'multiple', 'nuance', 'dish', 'meal', 'stun', 'dessert', 'sister', 'selection', 'target', 'bavarian', 'chocolate', 'hazelnut', 'tart', 'local', 'strawberries', 'served', 'buttery', 'crust', 'similar', 'mother', 'erika', 'tart', 'consist', 'creamy', 'whip', 'dark', 'chocolate', 'mousse', 'load', 'describe', 'chunky', 'hazelnut', 'butter', 'like', 'dark', 'nutella', 'pie', 'simplisticy', 'tart', 'match', 'decadence', 'dessert', 'decision', 'cheesecake', 'dish', 'server', 'describe', 'house', 'special', 'exceptionally', 'special', 'far', 'go', 'obvious', 'simple', 'decidedly', 'indulgent', 'rich', 'chocolate', 'cake', 'peach', 'ice', 'cream', 'served', 'like', 'keller', 'bouchon', 'dessert', 'thick', 'rich', 'warm', 'cake', 'top', 'large', 'scoop', 'ice', 'cream', 'taste', 'like', 'melt', 'peach', 'fuzz', 'focusing', 'cake', 'texture', 'steam', 'pudding', 'little', 'wet', 'set', 'hefty', 'bona', 'terra', 'experience', 'make', 'realize', 'good', 'largely', 'unmanipulated', 'fresh', 'fruit', 'vegetable', 'protein', 'place', 'hand', 'skilled', 'chef', 'great', 'food', 'wonderful', 'service', 'nice', 'location', 'hype', 'frill', 'hide', 'gem']\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df2['cleaned_text'].str.len().max())\n",
    "print(df2['cleaned_text'].str.len().mean())\n",
    "#len(df2.loc[df2['text'].str.len()==4518, 'text'].values[0])\n",
    "df2.loc[df2['cleaned_text'].str.len()==4518].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>These guys are awesome. I have been coming her...</td>\n",
       "      <td>en</td>\n",
       "      <td>['guy', 'awesome', 'come', 'year', 'restaurant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Stopped by on Tuesday to look for some Glock 2...</td>\n",
       "      <td>en</td>\n",
       "      <td>['stopped', 'tuesday', 'look', 'glock', 'mags'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Had a great experience here. Staff was really ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['great', 'experience', 'staff', 'nice', 'atte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This place is a complete joke. My fiancé order...</td>\n",
       "      <td>en</td>\n",
       "      <td>['place', 'complete', 'joke', 'fiancé', 'order...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Can you put more salt on you fries really my l...</td>\n",
       "      <td>en</td>\n",
       "      <td>['salt', 'fry', 'lip', 'burn', 'plz', 'okay', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99590</th>\n",
       "      <td>2</td>\n",
       "      <td>Came on BlogTO's recommendation. This restaura...</td>\n",
       "      <td>en</td>\n",
       "      <td>['came', 'blogto', 'recommendation', 'restaura...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99591</th>\n",
       "      <td>4</td>\n",
       "      <td>Ordered delivery to my hotel. They beat their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['ordered', 'delivery', 'hotel', 'beat', 'esti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99592</th>\n",
       "      <td>5</td>\n",
       "      <td>Deserves all five stars! Got all my questions ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['deserves', 'star', 'got', 'question', 'answe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99593</th>\n",
       "      <td>2</td>\n",
       "      <td>Let me cry for a second before I write this re...</td>\n",
       "      <td>en</td>\n",
       "      <td>['let', 'cry', '2', 'write', 'review', 'cry', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99594</th>\n",
       "      <td>1</td>\n",
       "      <td>Making this review two days out and still feel...</td>\n",
       "      <td>en</td>\n",
       "      <td>['making', 'review', 'day', 'feel', 'need', 'w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99595 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stars                                               text language  \\\n",
       "0          5  These guys are awesome. I have been coming her...       en   \n",
       "1          5  Stopped by on Tuesday to look for some Glock 2...       en   \n",
       "2          5  Had a great experience here. Staff was really ...       en   \n",
       "3          1  This place is a complete joke. My fiancé order...       en   \n",
       "4          1  Can you put more salt on you fries really my l...       en   \n",
       "...      ...                                                ...      ...   \n",
       "99590      2  Came on BlogTO's recommendation. This restaura...       en   \n",
       "99591      4  Ordered delivery to my hotel. They beat their ...       en   \n",
       "99592      5  Deserves all five stars! Got all my questions ...       en   \n",
       "99593      2  Let me cry for a second before I write this re...       en   \n",
       "99594      1  Making this review two days out and still feel...       en   \n",
       "\n",
       "                                            cleaned_text  sentiment  \n",
       "0      ['guy', 'awesome', 'come', 'year', 'restaurant...          1  \n",
       "1      ['stopped', 'tuesday', 'look', 'glock', 'mags'...          1  \n",
       "2      ['great', 'experience', 'staff', 'nice', 'atte...          1  \n",
       "3      ['place', 'complete', 'joke', 'fiancé', 'order...          0  \n",
       "4      ['salt', 'fry', 'lip', 'burn', 'plz', 'okay', ...          0  \n",
       "...                                                  ...        ...  \n",
       "99590  ['came', 'blogto', 'recommendation', 'restaura...          0  \n",
       "99591  ['ordered', 'delivery', 'hotel', 'beat', 'esti...          1  \n",
       "99592  ['deserves', 'star', 'got', 'question', 'answe...          1  \n",
       "99593  ['let', 'cry', '2', 'write', 'review', 'cry', ...          0  \n",
       "99594  ['making', 'review', 'day', 'feel', 'need', 'w...          0  \n",
       "\n",
       "[99595 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['sentiment'] = df2['stars'].apply(lambda x: 0 if x < 3 else 1)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DoSVsTuxxFB"
   },
   "source": [
    "Bag of words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMI3SHoK1rxy"
   },
   "outputs": [],
   "source": [
    "#todo: normalize words\n",
    "df2.shape\n",
    "positive = df2.head(1000).loc[(df2[\"sentiment\"] == 1)  ]\n",
    "negative = df2.head(1000).loc[(df2[\"sentiment\"] == 0)  ]\n",
    "print(positive.shape)\n",
    "print(negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-QticVl9_7s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guy': 974, 'awesome': 131, 'come': 429, 'year': 2482, 'restaurant': 1825, 'kitchen': 1170, 'wet': 2439, 'bar': 154, 'product': 1686, 'tv': 2316, 'help': 1013, 'bbq': 170, 'knife': 1171, 'set': 1934, 'like': 1228, 'business': 286, 'level': 1222, 'customer': 548, 'service': 1931, 'great': 952, 'send': 1925, 'client': 402, 'hear': 1004, 'wonderful': 2463, 'review': 1830, 'stopped': 2105, 'tuesday': 2311, 'look': 1251, 'soon': 2022, 'walk': 2402, 'welcome': 2435, 'ask': 102, 'need': 1417, 'minute': 1370, 'extremely': 764, 'helpful': 1014, 'definitely': 580, 'return': 1829, 'area': 88, 'experience': 756, 'staff': 2068, 'nice': 1426, 'attentive': 119, 'food': 852, 'delicious': 584, 'fresh': 876, 'recommend': 1767, 'place': 1600, 'complete': 448, 'joke': 1151, 'fiancé': 808, 'order': 1484, 'roll': 1851, 'happy': 992, 'hour': 1052, 'menu': 1340, 'regular': 1783, 'bring': 256, 'course': 505, 'take': 2174, 'sushi': 2156, 'average': 126, 'well': 2436, 'salt': 1878, 'fry': 888, 'burn': 283, 'okay': 1467, 'taste': 2188, 'purchase': 1711, 'tire': 2247, 'warranty': 2413, 'ago': 38, 'wife': 2446, 'leak': 1208, 'air': 41, 'try': 2308, 'car': 313, 'new': 1424, 'manager': 1295, 'type': 2319, 'excuse': 750, 'honor': 1040, 'etc': 733, 'discuss': 633, 'option': 1482, 'say': 1894, 'pay': 1547, 'state': 2083, 'spare': 2039, 'time': 2242, 'vehicle': 2375, 'home': 1035, 'safety': 1870, 'drive': 668, 'care': 316, 'discount': 631, 'nearby': 1412, 'show': 1960, 'sign': 1969, 'wall': 2404, 'bubble': 267, 'moment': 1382, 'tell': 2201, 'change': 348, 'note': 1442, 'close': 405, 'brand': 245, 'issue': 1132, 'certificate': 341, 'quickly': 1727, 'unlike': 2342, 'choose': 379, 'suggest': 2135, 'steer': 2092, 'clear': 398, 'lady': 1183, 'crazy': 519, 'receive': 1760, 'tip': 2246, 'demand': 590, 'cost': 498, 'body': 226, 'massage': 1310, 'hr': 1055, 'foot': 854, 'min': 1362, 'chair': 343, 'comfy': 432, 'snack': 2008, 'beverage': 197, 'give': 922, 'turn': 2315, 'disappoint': 627, 'good': 935, 'totally': 2272, 'chinese': 374, 'want': 2410, 'shake': 1938, 'went': 2437, 'location': 1246, 'friday': 878, 'night': 1428, 'wait': 2396, 'minimal': 1365, 'compare': 442, 'go': 929, 'early': 681, 'day': 563, 'ordered': 1485, 'burgers': 282, 'stack': 2066, 'smoke': 2005, 'spicy': 2049, 'pepper': 1558, 'pickle': 1582, 'sour': 2030, 'think': 2228, 'not': 1440, 'blend': 215, 'burger': 281, 'overall': 1496, 'salty': 1879, 'enjoy': 714, 'breading': 248, 'mushroom': 1399, 'stick': 2097, 'fall': 780, 'bite': 207, 'expensive': 755, 'wish': 2459, 'patty': 1546, 'cook': 487, 'love': 1261, 'medium': 1330, 'raw': 1747, 'weekend': 2431, 'lunch': 1271, 'buffet': 274, 'see': 1920, 'huge': 1057, 'selection': 1922, 'tasty': 2190, 'indian': 1091, 'southern': 2032, 'cuisine': 538, 'server': 1930, 'actually': 18, 'job': 1146, 'mention': 1339, 'atmosphere': 113, 'little': 1237, 'crowd': 532, 'usually': 2356, 'dine': 617, 'make': 1287, 'feel': 803, 'big': 200, 'city': 390, 'favorite': 798, 'madison': 1277, 'cheap': 355, 'choice': 378, 'rice': 1835, 'noodle': 1434, 'vegetable': 2371, 'combination': 426, 'dish': 635, 'fast': 792, 'hot': 1050, 'crispy': 528, 'overcook': 1497, 'today': 2250, 'express': 760, 'market': 1305, 'district': 639, 'know': 1174, 'explain': 758, 'enter': 717, 'grocery': 959, 'list': 1234, 'online': 1475, 'comfort': 430, 'wear': 2423, 'cart': 323, 'fill': 814, 'ice': 1067, 'cream': 520, 'cookie': 488, 'dough': 650, 'wine': 2453, 'schedule': 1899, 'pick': 1581, 'pull': 1706, 'let': 1219, 'shop': 1954, 'bag': 143, 'load': 1240, 'way': 2421, 'heart': 1005, 'adult': 26, 'wow': 2476, 'ready': 1752, 'door': 647, 'inside': 1109, 'super': 2143, 'jam': 1138, 'stuff': 2124, 'overwhelm': 1502, 'deep': 578, 'store': 2107, 'believe': 186, 'low': 1265, 'price': 1671, 'friend': 881, 'find': 820, 'friendly': 882, 'line': 1232, 'check': 356, 'key': 1161, 'trip': 2300, 'sale': 1873, 'happen': 990, 'buy': 291, 'free': 871, 'advantage': 28, 'cent': 336, 'cover': 509, 'get': 914, 'best': 194, 'soft': 2014, 'land': 1188, 'rush': 1866, 'wash': 2414, 'one': 1473, 'rid': 1837, 'tag': 2172, 'convince': 486, 'quality': 1719, 'depend': 595, 'worth': 2473, 'gamble': 897, 'properly': 1697, 'stand': 2073, 'counter': 501, 'sit': 1980, 'serve': 1929, 'table': 2168, 'horrible': 1045, 'family': 783, 'real': 1753, 'quick': 1726, 'busy': 287, 'lot': 1256, 'open': 1476, 'provide': 1702, 'leave': 1211, 'decide': 571, 'waiter': 2398, 'finally': 818, 'approach': 83, 'have': 998, 'thank': 2217, 'eat': 689, 'visit': 2389, 'vega': 2368, 'solid': 2016, 'simple': 1973, 'repair': 1802, 'oil': 1464, 'straight': 2109, 'forward': 863, 'mechanic': 1326, 'chain': 342, 'build': 276, 'model': 1377, 'push': 1714, 'lay': 1203, 'will': 2449, 'necessary': 1415, 'run': 1865, 'head': 1000, 'small': 2001, 'maintenance': 1284, 'pressure': 1664, 'unique': 2340, 'charlotte': 352, 'craft': 514, 'include': 1085, 'corner': 494, 'decor': 575, 'memorable': 1337, 'long': 1250, 'majority': 1286, 'rating': 1743, 'star': 2076, 'expect': 753, 'move': 1393, 'town': 2280, 'square': 2062, 'personally': 1570, 'old': 1468, 'offer': 1460, 'punch': 1709, 'people': 1557, 'game': 898, 'floor': 842, 'copy': 491, 'theme': 2223, 'date': 560, 'machine': 1275, 'maybe': 1317, 'add': 19, 'pool': 1630, 'underwhelm': 2334, 'hope': 1043, 'space': 2035, 'weekday': 2430, 'appetizer': 78, 'bummer': 277, 'follow': 851, 'reason': 1755, 'sauce': 1890, 'room': 1854, 'notice': 1443, 'chili': 371, 'cap': 312, 'girl': 920, 'boy': 241, 'son': 2020, 'high': 1020, 'kind': 1167, 'person': 1566, 'blame': 211, 'kid': 1163, 'bill': 202, 'stain': 2070, 'sad': 1867, 'professional': 1687, 'picture': 1585, 'dog': 644, 'pretty': 1667, 'amaze': 52, 'individual': 1093, 'personality': 1569, 'able': 0, 'shot': 1957, 'action': 15, 'month': 1385, 'puppy': 1710, 'listen': 1235, 'patient': 1543, 'grab': 941, 'attention': 118, 'focus': 848, 'shoot': 1953, 'truly': 2305, 'detail': 608, 'stage': 2069, 'idea': 1069, 'highly': 1022, 'work': 2467, 'future': 895, 'mediocre': 1328, 'noise': 1431, 'loud': 1258, 'music': 1401, 'ramen': 1735, 'thin': 2226, 'pork': 1636, 'belly': 188, 'sear': 1912, 'greasy': 951, 'heavy': 1009, 'ambience': 55, 'loved': 1262, 'white': 2444, 'chocolate': 377, 'peach': 1550, 'latte': 1200, 'bathroom': 166, 'cool': 490, 'outside': 1493, 'stay': 2087, 'groupon': 964, 'search': 1913, 'fun': 891, 'fitness': 828, 'class': 393, 'dance': 558, 'life': 1225, 'studio': 2122, 'far': 788, 'instructor': 1117, 'speak': 2040, 'suppose': 2148, 'step': 2094, 'encourage': 708, 'workout': 2469, 'late': 1198, 'student': 2121, 'opinion': 1478, 'money': 1384, 'spend': 2047, 'waited': 2397, 'shit': 1950, 'located': 1245, 'lobby': 1241, 'right': 1841, 'breakfast': 250, 'chicken': 368, 'season': 1914, 'complaint': 447, 'point': 1622, 'lemonade': 1216, 'sweet': 2160, 'tea': 2194, 'spot': 2057, 'dinner': 619, 'potato': 1643, 'pie': 1586, 'slightly': 1996, 'hard': 993, 'fish': 825, 'owner': 1505, 'falafel': 779, 'sandwich': 1884, 'appeal': 75, 'mushy': 1400, 'ball': 149, 'present': 1661, 'took': 2262, 'daughter': 561, 'hand': 985, 'watch': 2416, 'gentleman': 912, 'chat': 354, 'cell': 335, 'clearly': 399, 'personal': 1568, 'bowl': 239, 'manner': 1299, 'comment': 433, 'drink': 665, 'direction': 622, 'face': 768, 'raise': 1734, 'hey': 1017, 'man': 1292, 'blow': 220, 'smile': 2004, 'thanks': 2219, 'waste': 2415, 'convenient': 482, 'milk': 1357, 'boba': 224, 'literally': 1236, 'al': 43, 'mike': 1354, 'phenomenal': 1574, 'incredible': 1089, 'crab': 512, 'soup': 2029, 'thing': 2227, 'vibe': 2382, 'impeccable': 1075, 'die': 610, 'filet': 813, 'ribeye': 1834, 'absolutely': 2, 'easily': 685, 'double': 648, 'fantastic': 787, 'vegas': 2370, 'michael': 1348, 'fact': 771, 'morning': 1389, 'problem': 1681, 'dry': 671, 'reasonable': 1756, 'quote': 1729, 'polite': 1628, 'clean': 396, 'mill': 1359, 'young': 2493, 'luckily': 1268, 'found': 864, 'treat': 2293, 'main': 1281, 'special': 2041, 'select': 1921, 'value': 2362, 'hit': 1030, 'decent': 570, 'obviously': 1454, 'slow': 2000, 'nail': 1406, 'tech': 2199, 'unprofessional': 2345, 'instead': 1115, 'soak': 2009, 'finish': 823, 'dollar': 645, 'manicure': 1298, 'mani': 1297, 'pedi': 1553, 'presentation': 1662, 'completely': 449, 'budget': 272, 'reach': 1750, 'pocket': 1621, 'establishment': 731, 'scream': 1906, 'green': 954, 'jelly': 1143, 'taro': 2186, 'yum': 2496, 'top': 2265, 'pop': 1633, 'rainbow': 1733, 'oh': 1463, 'yes': 2487, 'wednesday': 2428, 'salad': 1872, 'design': 601, 'start': 2080, 'forget': 859, 'steak': 2088, 'rib': 1833, 'eye': 765, 'attach': 114, 'split': 2053, 'half': 980, 'tender': 2206, 'perfectly': 1562, 'side': 1967, 'lobster': 1242, 'mash': 1309, 'yummy': 2497, 'mac': 1274, 'cheese': 359, 'sprout': 2061, 'amazing': 53, 'dessert': 606, 'anymore': 65, 'butter': 288, 'cake': 298, 'unbelievable': 2329, 'french': 873, 'minor': 1367, 'cancel': 310, 'came': 305, 'window': 2452, 'payment': 1548, 'previously': 1670, 'agree': 39, 'credit': 524, 'card': 315, 'phone': 1578, 'call': 302, 'prior': 1677, 'post': 1641, 'poor': 1631, 'south': 2031, 'concern': 456, 'honestly': 1038, 'plain': 1601, 'mean': 1322, 'company': 440, 'wrong': 2479, 'shift': 1947, 'skill': 1986, 'advise': 31, 'arrive': 94, 'extra': 763, 'join': 1149, 'casual': 328, 'range': 1738, 'end': 709, 'sausage': 1891, 'slaw': 1990, 'engage': 711, 'brunch': 264, 'palace': 1519, 'child': 369, 'stop': 2104, 'house': 1053, 'seat': 1915, 'timely': 2243, 'exceed': 744, 'expectation': 754, 'typical': 2320, 'diner': 618, 'reheat': 1785, 'freeze': 872, 'hamburger': 984, 'generous': 911, 'size': 1983, 'sub': 2129, 'bread': 246, 'lastly': 1197, 'efficient': 694, 'appreciate': 82, 'especially': 728, 'perfect': 1560, 'fabulous': 767, 'beer': 182, 'variety': 2365, 'dirty': 625, 'veggies': 2374, 'worst': 2472, 'away': 130, 'train': 2286, 'generally': 909, 'plate': 1606, 'coffee': 416, 'filthy': 816, 'egg': 696, 'weak': 2422, 'water': 2417, 'hair': 978, 'cold': 418, 'break': 249, 'crisp': 527, 'rest': 1824, 'warm': 2411, 'hungry': 1060, 'attempt': 115, 'remake': 1791, 'fuck': 889, 'charge': 351, 'orange': 1483, 'juice': 1153, 'exact': 740, 'upset': 2350, 'shut': 1964, 'asap': 99, 'health': 1002, 'department': 594, 'pass': 1538, 'park': 1530, 'mobile': 1376, 'twice': 2317, 'week': 2429, 'penny': 1556, 'excellent': 745, 'world': 2470, 'scallop': 1895, 'king': 1169, 'event': 735, 'lately': 1199, 'bartender': 157, 'co': 409, 'word': 2466, 'husband': 1064, 'suck': 2133, 'pad': 1513, 'thai': 2216, 'organic': 1486, 'tofu': 2252, 'entire': 720, 'sticky': 2099, 'stomach': 2102, 'appear': 76, 'odd': 1459, 'shade': 1936, 'brown': 262, 'texture': 2215, 'resemble': 1814, 'chew': 365, 'flavor': 833, 'undercooked': 2331, 'carrot': 321, 'onion': 1474, 'peanut': 1552, 'cilantro': 387, 'balance': 148, 'spice': 2048, 'heat': 1006, 'benefit': 192, 'doubt': 649, 'reservation': 1815, 'risk': 1845, 'cafe': 296, 'girlfriend': 921, 'hype': 1066, 'thursday': 2237, 'lovely': 1263, 'name': 1407, 'strawberry': 2112, 'lime': 1230, 'overpower': 1500, 'calamari': 299, 'honest': 1037, 'pizza': 1599, 'pepperoni': 1559, 'wind': 2451, 'damn': 557, 'meat': 1324, 'simply': 1974, 'yesterday': 2488, 'chance': 346, 'numb': 1446, 'yelpers': 2486, 'america': 57, 'classic': 394, 'non': 1433, 'traditional': 2284, 'addition': 21, 'measure': 1323, 'challenge': 344, 'past': 1539, 'eventually': 736, 'settle': 1935, 'greek': 953, 'omelet': 1470, 'mix': 1375, 'gyro': 976, 'ingredient': 1102, 'hash': 995, 'toast': 2249, 'mind': 1363, 'group': 963, 'base': 158, 'speed': 2046, 'impressive': 1081, 'produce': 1685, 'venture': 2378, 'frequently': 875, 'catch': 330, 'remember': 1792, 'cash': 325, 'fee': 801, 'fat': 793, 'freshly': 877, 'buttery': 289, 'grill': 956, 'bun': 279, 'pile': 1589, 'beef': 181, 'keep': 1159, 'slider': 1994, 'fajitas': 777, 'consider': 468, 'street': 2113, 'school': 1900, 'flavorful': 834, 'garlic': 902, 'naan': 1404, 'live': 1238, 'impress': 1079, 'repeatedly': 1804, 'hunt': 1061, 'request': 1810, 'round': 1857, 'limit': 1231, 'fine': 821, 'case': 324, 'basic': 159, 'example': 743, 'awful': 132, 'pre': 1652, 'cut': 549, 'lettuce': 1221, 'dress': 664, 'short': 1955, 'lemon': 1215, 'parmesan': 1532, 'flavorless': 835, 'kick': 1162, 'wood': 2465, 'fire': 824, 'use': 2354, 'herb': 1016, 'olive': 1469, 'crust': 535, 'tasteless': 2189, 'topping': 2266, 'sorry': 2025, 'miss': 1373, 'replace': 1805, 'flip': 840, 'hospital': 1046, 'crack': 513, 'christmas': 384, 'teacher': 2196, 'lose': 1254, 'wonton': 2464, 'stir': 2100, 'roast': 1848, 'apart': 68, 'pasta': 1540, 'throw': 2235, 'dark': 559, 'gross': 962, 'phoenix': 1577, 'mesa': 1341, 'actual': 17, 'effort': 695, 'imagine': 1073, 'picky': 1584, 'rarely': 1740, 'comfortable': 431, 'booth': 233, 'philly': 1575, 'meal': 1321, 'glad': 923, 'local': 1243, 'own': 1504, 'cute': 550, 'claim': 391, 'vietnamese': 2384, 'minus': 1369, 'curry': 545, 'stew': 2096, 'basically': 160, 'drop': 670, 'encounter': 707, 'combo': 428, 'pho': 1576, 'mi': 1347, 'spring': 2060, 'crunchy': 534, 'large': 1192, 'broth': 259, 'dip': 620, 'read': 1751, 'improve': 1082, 'straw': 2111, 'napkin': 1408, 'plan': 1602, 'comparison': 443, 'kim': 1165, 'china': 373, 'afternoon': 35, 'even': 734, 'superb': 2144, 'wing': 2454, 'walmart': 2406, 'law': 1202, 'coworker': 510, 'single': 1976, 'last': 1196, 'item': 1134, 'cherry': 364, 'different': 613, 'sure': 2150, 'red': 1770, 'greet': 955, 'lead': 1207, 'hostess': 1049, 'proceed': 1683, 'decision': 573, 'deliver': 588, 'meet': 1331, 'lo': 1239, 'mein': 1333, 'ear': 680, 'soda': 2012, 'you': 2492, 'glass': 924, 'refill': 1774, 'pipe': 1593, 'ruin': 1863, 'absolute': 1, 'lack': 1182, 'mail': 1280, 'corporate': 495, 'contact': 475, 'direct': 621, 'apology': 72, 'sort': 2026, 'despite': 605, 'upscale': 2349, 'separate': 1927, 'waitstaff': 2400, 'cuz': 552, 'pack': 1511, 'bad': 141, 'sound': 2028, 'benedict': 191, 'poach': 1620, 'ham': 983, 'giant': 916, 'chunk': 385, 'appropriate': 84, 'slice': 1992, 'layer': 1204, 'matt': 1314, 'track': 2282, 'sadly': 1868, 'bacon': 140, 'portion': 1637, 'mini': 1364, 'blueberry': 222, 'muffin': 1396, 'messy': 1344, 'neck': 1416, 'strip': 2117, 'insult': 1118, 'pain': 1515, 'stars': 2079, 'general': 908, 'daily': 554, 'consistent': 471, 'aside': 101, 'certain': 339, 'baguette': 145, 'opportunity': 1479, 'shell': 1946, 'begin': 184, 'immediately': 1074, 'rude': 1861, 'couple': 503, 'stare': 2078, 'accommodate': 8, 'talk': 2177, 'story': 2108, 'power': 1649, 'relax': 1788, 'vacation': 2358, 'near': 1411, 'resort': 1819, 'party': 1537, 'fork': 860, 'certainly': 340, 'occur': 1457, 'fake': 778, 'nightmare': 1429, 'force': 856, 'road': 1847, 'incredibly': 1090, 'toilet': 2253, 'correct': 496, 'lesson': 1218, 'learn': 1209, 'delivery': 589, 'rave': 1745, 'process': 1684, 'possible': 1639, 'chandler': 347, 'locate': 1244, 'awkward': 134, 'unhappy': 2339, 'needless': 1418, 'favourite': 799, 'beat': 175, 'eggs': 698, 'book': 231, 'desk': 603, 'bed': 179, 'traffic': 2285, 'deposit': 596, 'confirmation': 464, 'bedroom': 180, 'available': 124, 'switch': 2164, 'sleep': 1991, 'refund': 1777, 'complain': 446, 'decline': 574, 'hold': 1031, 'hang': 989, 'warn': 2412, 'everybody': 737, 'save': 1892, 'hotel': 1051, 'confuse': 465, 'chicago': 367, 'pricey': 1673, 'mimosa': 1361, 'squeeze': 2064, 'almond': 48, 'croissant': 529, 'share': 1942, 'goat': 930, 'scramble': 1904, 'candy': 311, 'montreal': 1387, 'beautiful': 176, 'question': 1725, 'dairy': 555, 'answer': 63, 'knowledgeable': 1177, 'conversation': 485, 'interrupt': 1123, 'bus': 285, 'boyfriend': 242, 'mom': 1381, 'allow': 47, 'professionalism': 1688, 'inexpensive': 1097, 'bother': 236, 'fix': 829, 'kinda': 1168, 'scene': 1898, 'guess': 970, 'tacos': 2170, 'barely': 156, 'understand': 2333, 'vegan': 2369, 'difficult': 614, 'rip': 1843, 'dozen': 656, 'plus': 1618, 'environment': 723, 'black': 209, 'bear': 174, 'exceptional': 747, 'gourmet': 940, 'appetite': 77, 'accord': 10, 'rate': 1742, 'negative': 1419, 'report': 1808, 'chef': 363, 'deny': 593, 'employee': 704, 'laugh': 1201, 'management': 1294, 'margarita': 1302, 'bean': 173, 'salsa': 1877, 'chip': 375, 'weird': 2434, 'paper': 1526, 'towel': 2278, 'patio': 1544, 'furniture': 893, 'uncomfortable': 2330, 'ceil': 333, 'enchilada': 705, 'seafood': 1911, 'enchiladas': 706, 'edge': 692, 'probably': 1680, 'fan': 785, 'tomato': 2255, 'typically': 2321, 'bland': 212, 'spoon': 2055, 'bake': 146, 'soggy': 2015, 'float': 841, 'tattoo': 2191, 'knowledge': 1176, 'pleasant': 1611, 'attitude': 120, 'uneven': 2335, 'diamond': 609, 'everytime': 739, 'satisfy': 1888, 'avocado': 127, 'industry': 1095, 'promise': 1691, 'chipotle': 376, 'membership': 1336, 'invite': 1126, 'looking': 1252, 'shape': 1941, 'unfortunately': 2337, 'holiday': 1033, 'play': 1608, 'rough': 1856, 'facility': 770, 'animal': 60, 'to': 2248, 'alot': 49, 'pet': 1572, 'supervisor': 2145, 'explanation': 759, 'member': 1335, 'positive': 1638, 'handle': 987, 'aware': 129, 'judge': 1152, 'skin': 1987, 'court': 506, 'clue': 408, 'system': 2166, 'condescend': 458, 'office': 1462, 'worker': 2468, 'hate': 997, 'can': 307, 'be': 172, 'deserve': 600, 'authentic': 122, 'walked': 2403, 'wander': 2408, 'tamale': 2179, 'taco': 2169, 'cup': 541, 'waitress': 2399, 'fountain': 865, 'ahead': 40, 'register': 1781, 'realize': 1754, 'whatsoever': 2440, 'difference': 612, 'overpriced': 1501, 'dim': 616, 'sum': 2139, 'eggplant': 697, 'duck': 672, 'cod': 414, 'champagne': 345, 'honey': 1039, 'cocktail': 412, 'strong': 2118, 'prefer': 1653, 'recent': 1761, 'fault': 796, 'mexican': 1346, 'tow': 2277, 'accident': 7, 'inch': 1084, 'paperwork': 1527, 'transfer': 2289, 'prove': 1701, 'trouble': 2301, 'shirt': 1949, 'address': 23, 'driver': 669, 'license': 1223, 'human': 1058, 'steal': 2090, 'cause': 332, 'anyways': 67, 'continue': 478, 'drinks': 666, 'shrimp': 1963, 'quesadilla': 1723, 'las': 1193, 'quantity': 1720, 'cleanliness': 397, 'doctor': 643, 'deal': 566, 'nurse': 1449, 'exam': 742, 'exactly': 741, 'information': 1100, 'bitter': 208, 'trump': 2306, 'gas': 903, 'healthy': 1003, 'trendy': 2296, 'prepare': 1659, 'tried': 2298, 'ok': 1466, 'rise': 1844, 'planet': 1603, 'figure': 811, 'incompetent': 1086, 'argue': 89, 'better': 196, 'additional': 22, 'discover': 632, 'stock': 2101, 'syrup': 2165, 'sugar': 2134, 'sam': 1880, 'plenty': 1616, 'personable': 1567, 'mid': 1350, 'sister': 1979, 'brother': 260, 'platter': 1607, 'pleasure': 1614, 'layout': 1205, 'grind': 957, 'hide': 1019, 'rob': 1849, 'pesto': 1571, 'prime': 1675, 'spinach': 2052, 'cab': 293, 'consistency': 470, 'exception': 746, 'salon': 1876, 'stylist': 2128, 'anytime': 66, 'mango': 1296, 'smoothie': 2007, 'twist': 2318, 'corn': 492, 'mayo': 1318, 'saturday': 1889, 'yelp': 2485, 'easy': 688, 'valet': 2360, 'mall': 1291, 'chewy': 366, 'enjoyable': 715, 'creamy': 521, 'chris': 383, 'win': 2450, 'entree': 722, 'bone': 229, 'juicy': 1154, 'center': 337, 'char': 349, 'piece': 1587, 'complimentary': 452, 'tool': 2263, 'cater': 331, 'supply': 2146, 'frustrate': 887, 'insurance': 1119, 'medical': 1327, 'fried': 880, 'thick': 2225, 'tough': 2274, 'cabbage': 294, 'tend': 2205, 'require': 1811, 'disappointment': 629, 'overly': 1499, 'research': 1813, 'sink': 1977, 'got': 939, 'joint': 1150, 'plastic': 1605, 'metal': 1345, 'replacement': 1806, 'resolve': 1818, 'potential': 1644, 'response': 1822, 'accept': 4, 'drain': 660, 'total': 2271, 'neighbor': 1420, 'apparently': 74, 'essentially': 730, 'situation': 1982, 'appointment': 81, 'initially': 1104, 'till': 2241, 'monday': 1383, 'slight': 1995, 'oven': 1495, 'basil': 161, 'ny': 1451, 'light': 1227, 'subway': 2132, 'rock': 1850, 'excite': 749, 'terribly': 2210, 'waffle': 2395, 'skip': 1988, 'smell': 2003, 'draw': 661, 'sample': 1881, 'chop': 380, 'lots': 1257, 'burrito': 284, 'ring': 1842, 'unable': 2327, 'attend': 116, 'advertise': 29, 'email': 701, 'version': 2380, 'garden': 901, 'cross': 530, 'guarantee': 968, 'basket': 163, 'flower': 845, 'arrangement': 92, 'vendor': 2376, 'board': 223, 'describe': 597, 'veggie': 2373, 'lamb': 1185, 'pink': 1592, 'tiny': 2245, 'walnut': 2407, 'grow': 965, 'alright': 50, 'spin': 2051, 'woman': 2461, 'please': 1613, 'nearly': 1413, 'host': 1048, 'flag': 830, 'drench': 663, 'mussel': 1402, 'edible': 693, 'nigiri': 1430, 'par': 1528, 'leftover': 1212, 'bore': 234, 'pace': 1510, 'quiet': 1728, 'downtown': 655, 'reflect': 1775, 'act': 14, 'spaghetti': 2037, 'de': 564, 'cozy': 511, 'collection': 422, 'japanese': 1140, 'interior': 1121, 'refresh': 1776, 'multiple': 1397, 'dumpling': 676, 'delightful': 586, 'chile': 370, 'knock': 1172, 'beware': 198, 'north': 1439, 'scottsdale': 1903, 'mile': 1356, 'chorizo': 381, 'biscuit': 204, 'homemade': 1036, 'pancake': 1522, 'outstanding': 1494, 'tapa': 2183, 'test': 2212, 'similar': 1972, 'section': 1918, 'travel': 2291, 'mountain': 1391, 'guest': 971, 'second': 1916, 'view': 2385, 'roof': 1853, 'modern': 1378, 'amenity': 56, 'true': 2303, 'error': 726, 'damage': 556, 'property': 1698, 'account': 11, 'preference': 1654, 'queen': 1722, 'renovate': 1797, 'tower': 2279, 'carpet': 320, 'tear': 2198, 'toy': 2281, 'sheet': 1945, 'visitor': 2390, 'disgust': 634, 'pm': 1619, 'housekeeping': 1054, 'ugh': 2324, 'ridiculously': 1840, 'ac': 3, 'gelato': 906, 'summer': 2140, 'study': 2123, 'bakery': 147, 'contract': 479, 'estimate': 732, 'steve': 2095, 'remove': 1795, 'filter': 815, 'ridiculous': 1839, 'standard': 2074, 'bit': 206, 'fruit': 886, 'tax': 2192, 'reasonably': 1757, 'gem': 907, 'pot': 1642, 'sticker': 2098, 'wake': 2401, 'quaint': 1718, 'charm': 353, 'inquire': 1106, 'minimum': 1366, 'score': 1902, 'app': 73, 'current': 543, 'assure': 112, 'status': 2086, 'news': 1425, 'policy': 1626, 'sense': 1926, 'rental': 1800, 'god': 931, 'lucky': 1269, 'treatment': 2294, 'inform': 1099, 'tone': 2258, 'indicate': 1092, 'forever': 858, 'grand': 944, 'sip': 1978, 'convenience': 481, 'omg': 1472, 'korean': 1178, 'affordable': 33, 'slam': 1989, 'plentiful': 1615, 'putt': 1717, 'exit': 752, 'california': 301, 'canada': 308, 'offering': 1461, 'ambiance': 54, 'ginger': 919, 'calgary': 300, 'wave': 2419, 'guacamole': 967, 'battery': 168, 'iphone': 1128, 'apple': 79, 'heck': 1010, 'blah': 210, 'tho': 2229, 'mad': 1276, 'bet': 195, 'screw': 1908, 'freak': 870, 'update': 2347, 'wifi': 2447, 'outlet': 1491, 'hook': 1041, 'sunday': 2142, 'support': 2147, 'artist': 97, 'community': 437, 'fare': 789, 'trade': 2283, 'period': 1565, 'yr': 2494, 'gal': 896, 'boss': 235, 'dr': 657, 'mess': 1342, 'hello': 1012, 'lazy': 1206, 'promptly': 1694, 'scratch': 1905, 'fiance': 807, 'reply': 1807, 'part': 1533, 'shred': 1962, 'pour': 1646, 'lousy': 1260, 'bud': 270, 'procedure': 1682, 'legit': 1214, 'farm': 790, 'artichoke': 96, 'octopus': 1458, 'skewer': 1985, 'udon': 2323, 'notch': 1441, 'cleveland': 401, 'popular': 1635, 'upstairs': 2351, 'downstairs': 654, 'interest': 1120, 'decorate': 576, 'couch': 499, 'delight': 585, 'mozzarella': 1395, 'fancy': 786, 'fair': 775, 'bottle': 237, 'sun': 2141, 'starter': 2081, 'trash': 2290, 'apologetic': 70, 'seriously': 1928, 'female': 805, 'badly': 142, 'called': 303, 'anybody': 64, 'rent': 1799, 'technician': 2200, 'inconvenience': 1087, 'shower': 1961, 'elevator': 700, 'ins': 1107, 'gate': 904, 'garage': 899, 'responsibility': 1823, 'assistant': 109, 'important': 1076, 'write': 2478, 'notify': 1444, 'police': 1625, 'directly': 623, 'emergency': 703, 'neighborhood': 1421, 'recently': 1762, 'obvious': 1453, 'target': 2185, 'lock': 1247, 'afraid': 34, 'activity': 16, 'st': 2065, 'bang': 152, 'rub': 1859, 'parking': 1531, 'football': 855, 'terrible': 2209, 'crave': 518, 'flour': 843, 'dump': 675, 'flat': 832, 'desperate': 604, 'promotion': 1692, 'assume': 111, 'irritate': 1130, 'donut': 646, 'bagel': 144, 'currently': 544, 'background': 139, 'ignore': 1071, 'repeat': 1803, 'code': 415, 'toronto': 2267, 'avoid': 128, 'pub': 1703, 'cramp': 515, 'likely': 1229, 'pregnant': 1655, 'sick': 1966, 'worthy': 2474, 'cucumber': 537, 'poison': 1623, 'apologize': 71, 'solution': 2017, 'angry': 59, 'touch': 2273, 'screen': 1907, 'boot': 232, 'normally': 1438, 'understaffed': 2332, 'wipe': 2457, 'favor': 797, 'gift': 917, 'hut': 1065, 'hopefully': 1044, 'tune': 2313, 'oily': 1465, 'breast': 251, 'pretzel': 1668, 'style': 2127, 'spectacular': 2045, 'yellow': 2484, 'bright': 255, 'mold': 1380, 'genuinely': 913, 'teriyaki': 2207, 'resident': 1817, 'hire': 1028, 'consist': 469, 'restroom': 1826, 'clothe': 406, 'tour': 2275, 'surprise': 2153, 'history': 1029, 'scare': 1897, 'matter': 1315, 'az': 136, 'form': 861, 'info': 1098, 'shame': 1939, 'happily': 991, 'moist': 1379, 'sesame': 1932, 'batter': 167, 'sashimi': 1886, 'degree': 581, 'tail': 2173, 'spill': 2050, 'station': 2085, 'ownership': 1506, 'embarrass': 702, 'regard': 1779, 'poorly': 1632, 'conveniently': 483, 'pathetic': 1542, 'hardly': 994, 'flamingo': 831, 'celebrate': 334, 'upgrade': 2348, 'lake': 1184, 'swim': 2162, 'movie': 1394, 'id': 1068, 'rend': 1796, 'print': 1676, 'assist': 107, 'equipment': 725, 'club': 407, 'diva': 640, 'concert': 457, 'mediterranean': 1329, 'major': 1285, 'organize': 1487, 'opposite': 1480, 'john': 1148, 'west': 2438, 'surgery': 2152, 'surprisingly': 2154, 'clinic': 404, 'salmon': 1875, 'lounge': 1259, 'chill': 372, 'mattress': 1316, 'luck': 1267, 'extend': 761, 'representative': 1809, 'control': 480, 'trust': 2307, 'language': 1190, 'henderson': 1015, 'particular': 1534, 'previous': 1669, 'powder': 1648, 'mouth': 1392, 'pittsburgh': 1598, 'hip': 1027, 'slip': 1997, 'having': 999, 'sport': 2056, 'player': 1609, 'ton': 2257, 'vary': 2366, 'tab': 2167, 'hassle': 996, 'renovation': 1798, 'color': 424, 'tank': 2181, 'mother': 1390, 'ill': 1072, 'refuse': 1778, 'spa': 2034, 'acrylic': 13, 'gel': 905, 'polish': 1627, 'redo': 1772, 'quarter': 1721, 'crappy': 517, 'hilton': 1025, 'awhile': 133, 'smooth': 2006, 'san': 1883, 'occupy': 1456, 'yell': 2483, 'suite': 2138, 'cigarette': 386, 'golf': 934, 'tray': 2292, 'spacious': 2036, 'rain': 1732, 'admit': 25, 'feed': 802, 'proper': 1696, 'mood': 1388, 'comped': 444, 'internet': 1122, 'laptop': 1191, 'jewelry': 1145, 'wed': 2426, 'ticket': 2238, 'computer': 453, 'bunch': 280, 'nut': 1450, 'asian': 100, 'ship': 1948, 'receipt': 1759, 'el': 699, 'tap': 2182, 'queso': 1724, 'nachos': 1405, 'ride': 1838, 'hike': 1023, 'rep': 1801, 'comp': 438, 'entrance': 721, 'hall': 981, 'coleslaw': 419, 'usual': 2355, 'thrill': 2234, 'race': 1730, 'practice': 1651, 'strange': 2110, 'fail': 774, 'guac': 966, 'zero': 2498, 'pro': 1679, 'strike': 2116, 'dealership': 568, 'hell': 1011, 'ass': 106, 'dude': 673, 'lie': 1224, 'block': 216, 'sell': 1924, 'grade': 943, 'turkey': 2314, 'blue': 221, 'highlight': 1021, 'parent': 1529, 'bay': 169, 'barber': 155, 'button': 290, 'famous': 784, 'coupon': 504, 'marry': 1306, 'july': 1155, 'deli': 583, 'annoy': 62, 'debit': 569, 'la': 1179, 'hill': 1024, 'pastry': 1541, 'recommendation': 1768, 'caramel': 314, 'labor': 1181, 'overlook': 1498, 'downside': 653, 'casino': 327, 'frank': 868, 'middle': 1351, 'fellow': 804, 'reserve': 1816, 'age': 36, 'takeout': 2175, 'message': 1343, 'midnight': 1352, 'rudely': 1862, 'loss': 1255, 'equally': 724, 'rare': 1739, 'occasion': 1455, 'fit': 827, 'relatively': 1787, 'stupid': 2126, 'specialty': 2042, 'tomorrow': 2256, 'self': 1923, 'golden': 933, 'respect': 1820, 'theirs': 2222, 'condiment': 459, 'hole': 1032, 'wild': 2448, 'performance': 1564, 'install': 1113, 'condition': 460, 'maintain': 1283, 'tempe': 2202, 'smart': 2002, 'cashier': 326, 'canadian': 309, 'poutine': 1647, 'gravy': 949, 'pea': 1549, 'description': 598, 'photo': 1579, 'mustard': 1403, 'box': 240, 'groom': 960, 'introduce': 1125, 'tooth': 2264, 'unpleasant': 2344, 'website': 2425, 'hibachi': 1018, 'entertain': 718, 'theatre': 2221, 'pedicure': 1554, 'toe': 2251, 'stone': 2103, 'fight': 810, 'chow': 382, 'site': 1981, 'original': 1488, 'random': 1737, 'soy': 2033, 'beautifully': 177, 'gratuity': 948, 'unfriendly': 2338, 'shuttle': 1965, 'flight': 839, 'airport': 42, 'pickup': 1583, 'confirm': 463, 'result': 1827, 'wide': 2445, 'shoe': 1952, 'buck': 268, 'unfortunate': 2336, 'feta': 806, 'omelette': 1471, 'grease': 950, 'pita': 1596, 'instruction': 1116, 'ford': 857, 'island': 1131, 'guide': 972, 'constantly': 473, 'tony': 2261, 'cone': 461, 'circle': 389, 'van': 2363, 'correctly': 497, 'march': 1301, 'operate': 1477, 'pleasantly': 1612, 'fusion': 894, 'specifically': 2044, 'meh': 1332, 'bomb': 228, 'sloppy': 1998, 'initial': 1103, 'mcdonald': 1319, 'english': 713, 'melt': 1334, 'pudding': 1705, 'romantic': 1852, 'leg': 1213, 'temperature': 2203, 'shoulder': 1958, 'prescription': 1660, 'asked': 103, 'program': 1689, 'common': 434, 'patron': 1545, 'consistently': 472, 'american': 58, 'nasty': 1409, 'tonight': 2260, 'fly': 847, 'shock': 1951, 'suggestion': 2136, 'attendant': 117, 'auto': 123, 'stellar': 2093, 'create': 522, 'goodness': 936, 'soul': 2027, 'vegetarian': 2372, 'draft': 658, 'memory': 1338, 'lane': 1189, 'caesar': 295, 'blood': 218, 'brick': 254, 'art': 95, 'calm': 304, 'dream': 662, 'combine': 427, 'asparagus': 104, 'fully': 890, 'associate': 110, 'sweat': 2159, 'sake': 1871, 'stale': 2072, 'coast': 410, 'recipe': 1765, 'bf': 199, 'pretend': 1665, 'hurry': 1062, 'lol': 1249, 'wonder': 2462, 'creative': 523, 'supposedly': 2149, 'voice': 2392, 'stress': 2114, 'press': 1663, 'particularly': 1535, 'rule': 1864, 'advance': 27, 'culture': 540, 'nope': 1436, 'bug': 275, 'inspection': 1111, 'band': 151, 'root': 1855, 'exist': 751, 'garbage': 900, 'complex': 450, 'suit': 2137, 'lease': 1210, 'pant': 1525, 'apartment': 69, 'thorough': 2230, 'paint': 1517, 'wrap': 2477, 'gym': 975, 'safe': 1869, 'extensive': 762, 'prices': 1672, 'baby': 137, 'haircut': 979, 'gun': 973, 'package': 1512, 'soooo': 2024, 'perfection': 1561, 'fluffy': 846, 'record': 1769, 'handful': 986, 'liquor': 1233, 'desert': 599, 'weight': 2433, 'tot': 2270, 'shave': 1943, 'remind': 1793, 'dad': 553, 'entertainment': 719, 'factor': 772, 'earn': 682, 'gluten': 927, 'gf': 915, 'shady': 1937, 'meatball': 1325, 'drip': 667, 'rubbery': 1860, 'contain': 476, 'spray': 2058, 'team': 2197, 'construction': 474, 'stumble': 2125, 'coconut': 413, 'martini': 1307, 'indoor': 1094, 'outdoor': 1490, 'vip': 2388, 'terrific': 2211, 'compliment': 451, 'fairly': 776, 'weather': 2424, 'arrival': 93, 'possibly': 1640, 'silverware': 1971, 'weekly': 2432, 'toss': 2269, 'classy': 395, 'beauty': 178, 'hurt': 1063, 'decided': 572, 'btw': 266, 'mark': 1304, 'watery': 2418, 'creme': 525, 'allergy': 46, 'wise': 2458, 'whip': 2442, 'yuck': 2495, 'surely': 2151, 'bell': 187, 'tortilla': 2268, 'nyc': 1452, 'italian': 1133, 'eatery': 691, 'destination': 607, 'wedge': 2427, 'dragon': 659, 'brewery': 253, 'pound': 1645, 'inspire': 1112, 'birthday': 203, 'session': 1933, 'bank': 153, 'mistake': 1374, 'bouchon': 238, 'bloody': 219, 'mary': 1308, 'gorgeous': 938, 'tree': 2295, 'storage': 2106, 'container': 477, 'number': 1447, 'ranch': 1736, 'rich': 1836, 'prompt': 1693, 'ave': 125, 'alcohol': 44, 'aspect': 105, 'truck': 2302, 'worry': 2471, 'length': 1217, 'expire': 757, 'nicely': 1427, 'male': 1290, 'regardless': 1780, 'pic': 1580, 'foie': 849, 'gras': 945, 'ensure': 716, 'carne': 318, 'asada': 98, 'carry': 322, 'ayce': 135, 'steam': 2091, 'kimchi': 1166, 'jerk': 1144, 'colleague': 420, 'david': 562, 'video': 2383, 'reliable': 1789, 'arm': 91, 'wheel': 2441, 'hop': 1042, 'dumb': 674, 'plaza': 1610, 'cupcake': 542, 'cheesecake': 361, 'cooky': 489, 'tape': 2184, 'dj': 641, 'savory': 1893, 'hint': 1026, 'somewhat': 2019, 'hummus': 1059, 'natural': 1410, 'trim': 2299, 'peel': 1555, 'finger': 822, 'communicate': 435, 'public': 1704, 'banana': 150, 'pineapple': 1591, 'tourist': 2276, 'character': 350, 'tuna': 2312, 'sea': 1910, 'pan': 1521, 'sooo': 2023, 'insist': 1110, 'dirt': 624, 'delay': 582, 'shout': 1959, 'energy': 710, 'sofa': 2013, 'desire': 602, 'concept': 455, 'dentist': 592, 'swear': 2158, 'slot': 1999, 'crap': 516, 'dealer': 567, 'teach': 2195, 'folk': 850, 'crunch': 533, 'jack': 1135, 'delish': 587, 'arizona': 90, 'brisket': 257, 'panda': 1523, 'hands': 988, 'salesman': 1874, 'headache': 1001, 'wings': 2455, 'eater': 690, 'wax': 2420, 'generic': 910, 'cinnamon': 388, 'joe': 1147, 'pumpkin': 1708, 'specific': 2043, 'intimate': 1124, 'brew': 252, 'crow': 531, 'escape': 727, 'therapist': 2224, 'truffle': 2304, 'alternative': 51, 'buffalo': 273, 'steakhouse': 2089, 'courteous': 507, 'coke': 417, 'wanna': 2409, 'clientele': 403, 'pitcher': 1597, 'nervous': 1422, 'york': 2491, 'country': 502, 'connect': 466, 'fashion': 791, 'funny': 892, 'slide': 1993, 'january': 1139, 'million': 1360, 'ugly': 2325, 'distance': 638, 'access': 6, 'palms': 1520, 'label': 1180, 'knot': 1173, 'dig': 615, 'pushy': 1715, 'stair': 2071, 'tight': 2239, 'cat': 329, 'secret': 1917, 'manage': 1293, 'term': 2208, 'thousand': 2233, 'put': 1716, 'yeah': 2481, 'respond': 1821, 'microwave': 1349, 'maker': 1288, 'utensil': 2357, 'coat': 411, 'pig': 1588, 'substitute': 2131, 'protein': 1700, 'glove': 926, 'remodel': 1794, 'vet': 2381, 'thankfully': 2218, 'noon': 1435, 'cajun': 297, 'breaded': 247, 'bc': 171, 'def': 579, 'back': 138, 'camera': 306, 'trick': 2297, 'tuck': 2310, 'eastern': 687, 'partner': 1536, 'miso': 1372, 'originally': 1489, 'haha': 977, 'uber': 2322, 'east': 686, 'spanish': 2038, 'wtf': 2480, 'reviewer': 1831, 'oz': 1509, 'tall': 2178, 'oyster': 1508, 'spread': 2059, 'thought': 2232, 'bath': 165, 'facial': 769, 'fries': 883, 'cheeseburger': 360, 'mild': 1355, 'pure': 1712, 'flavour': 836, 'father': 794, 'broccoli': 258, 'cheddar': 358, 'exchange': 748, 'clam': 392, 'fridge': 879, 'blanket': 213, 'pillow': 1590, 'pharmacy': 1573, 'regularly': 1784, 'berry': 193, 'maple': 1300, 'cheesy': 362, 'tempura': 2204, 'starbucks': 2077, 'communication': 436, 'display': 636, 'doughnut': 651, 'sketchy': 1984, 'prep': 1657, 'acknowledge': 12, 'jump': 1156, 'brownie': 263, 'veal': 2367, 'frites': 884, 'private': 1678, 'struggle': 2120, 'staple': 2075, 'anniversary': 61, 'finance': 819, 'disappear': 626, 'soap': 2010, 'shampoo': 1940, 'stretch': 2115, 'vanilla': 2364, 'ultimately': 2326, 'familiar': 782, 'plug': 1617, 'flow': 844, 'informative': 1101, 'opt': 1481, 'duty': 678, 'belt': 189, 'unlimited': 2343, 'basis': 162, 'remain': 1790, 'strongly': 2119, 'scoop': 1901, 'would': 2475, 'numerous': 1448, 'noisy': 1432, 'loyal': 1266, 'shortly': 1956, 'unacceptable': 2328, 'grass': 946, 'tan': 2180, 'gold': 932, 'receptionist': 1764, 'redeem': 1771, 'assistance': 108, 'yogurt': 2490, 'addict': 20, 'tad': 2171, 'dust': 677, 'sampler': 1882, 'companion': 439, 'accompany': 9, 'sing': 1975, 'volume': 2393, 'wallet': 2405, 'jalapeno': 1136, 'mint': 1368, 'transaction': 2288, 'premium': 1656, 'thoroughly': 2231, 'diet': 611, 'uptown': 2352, 'grace': 942, 'final': 817, 'purpose': 1713, 'subpar': 2130, 'squash': 2063, 'mcdonalds': 1320, 'pretentious': 1666, 'impression': 1080, 'bass': 164, 'zucchini': 2499, 'vacuum': 2359, 'pair': 1518, 'cousin': 508, 'preparation': 1658, 'text': 2214, 'project': 1690, 'security': 1919, 'brake': 243, 'practically': 1650, 'starve': 2082, 'valley': 2361, 'pit': 1595, 'up': 2346, 'fatty': 795, 'involve': 1127, 'jalapeño': 1137, 'sat': 1887, 'dental': 591, 'adjust': 24, 'peak': 1551, 'everyday': 738, 'bike': 201, 'trainer': 2287, 'surround': 2155, 'hubby': 1056, 'field': 809, 'pump': 1707, 'engine': 712, 'politely': 1629, 'impossible': 1078, 'lukewarm': 1270, 'frequent': 874, 'cry': 536, 'loose': 1253, 'august': 121, 'approximately': 85, 'advice': 30, 'custom': 547, 'texas': 2213, 'carnitas': 319, 'importantly': 1077, 'guard': 969, 'poke': 1624, 'flavourful': 837, 'file': 812, 'thumb': 2236, 'brush': 265, 'frankly': 869, 'bistro': 205, 'painful': 1516, 'scam': 1896, 'downhill': 652, 'venue': 2379, 'rd': 1749, 'row': 1858, 'regret': 1782, 'factory': 773, 'theater': 2220, 'museum': 1398, 'neat': 1414, 'iron': 1129, 'ease': 684, 'oxtail': 1507, 'dye': 679, 'lamp': 1187, 'boil': 227, 'apply': 80, 'mirror': 1371, 'tub': 2309, 'massive': 1311, 'hallway': 982, 'tongue': 2259, 'normal': 1437, 'sight': 1968, 'match': 1313, 'pride': 1674, 'count': 500, 'culinary': 539, 'magic': 1278, 'plant': 1604, 'gnocchi': 928, 'reschedule': 1812, 'page': 1514, 'recognize': 1766, 'reward': 1832, 'acceptable': 5, 'heaven': 1008, 'kill': 1164, 'dmv': 642, 'sweetness': 2161, 'increase': 1088, 'song': 2021, 'hospitality': 1047, 'retail': 1828, 'makeup': 1289, 'risotto': 1846, 'alfredo': 45, 'lasagna': 1194, 'recall': 1758, 'social': 2011, 'afford': 32, 'bench': 190, 'whiskey': 2443, 'bobby': 225, 'flay': 838, 'swiss': 2163, 'stadium': 2067, 'popcorn': 1634, 'mainly': 1282, 'blast': 214, 'careful': 317, 'inn': 1105, 'refer': 1773, 'espresso': 729, 'ratio': 1744, 'venetian': 2377, 'luxury': 1272, 'disrespectful': 637, 'lame': 1186, 'statement': 2084, 'colour': 425, 'idiot': 1070, 'grit': 958, 'unit': 2341, 'voucher': 2394, 'competition': 445, 'conference': 462, 'dead': 565, 'cornbread': 493, 'lv': 1273, 'suspect': 2157, 'spoil': 2054, 'con': 454, 'pros': 1699, 'cons': 467, 'improvement': 1083, 'fool': 853, 'disaster': 630, 'jar': 1141, 'earth': 683, 'fishy': 826, 'grateful': 947, 'ray': 1748, 'college': 423, 'panini': 1524, 'buyer': 292, 'bonus': 230, 'inedible': 1096, 'knowledgable': 1175, 'behavior': 185, 'proof': 1695, 'tart': 2187, 'central': 338, 'false': 781, 'appt': 86, 'nugget': 1445, 'heater': 1007, 'master': 1312, 'installation': 1114, 'collect': 421, 'cuticle': 551, 'scrub': 1909, 'winter': 2456, 'insane': 1108, 'rack': 1731, 'tile': 2240, 'urban': 2353, 'milkshake': 1358, 'letter': 1220, 'feature': 800, 'network': 1423, 'monthly': 1386, 'tint': 2244, 'lover': 1264, 'piss': 1594, 'mignon': 1353, 'outrageous': 1492, 'ketchup': 1160, 'kale': 1158, 'holy': 1034, 'blonde': 217, 'beet': 183, 'owe': 1503, 'branch': 244, 'clerk': 400, 'signature': 1970, 'franchise': 867, 'convention': 484, 'lift': 1226, 'frame': 866, 'google': 937, 'frost': 885, 'vintage': 2387, 'taxi': 2193, 'talented': 2176, 'sangria': 1885, 'bump': 278, 'bucket': 269, 'crepe': 526, 'tom': 2254, 'mahi': 1279, 'ravioli': 1746, 'raspberry': 1741, 'decoration': 577, 'marinate': 1303, 'glaze': 925, 'agent': 37, 'reception': 1763, 'witness': 2460, 'custard': 546, 'comparable': 441, 'forth': 862, 'eyebrow': 766, 'vinegar': 2386, 'checkout': 357, 'gilbert': 918, 'perform': 1563, 'shawarma': 1944, 'kabob': 1157, 'somebody': 2018, 'buddy': 271, 'lash': 1195, 'relate': 1786, 'groomer': 961, 'jason': 1142, 'locker': 1248, 'vodka': 2391, 'disappointed': 628, 'brow': 261, 'yoga': 2489, 'arcade': 87}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-badc540ef7b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# encode document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mXX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# summarize encoded vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "#df2['cleaned_text'] = df2['cleaned_text'].astype(str)\n",
    "positive_review = [\"The restaurant was great. The food was delicious and the staff was extremely kind.\"]\n",
    "negative_review = [\"The restaurant was awful. The food was cold.\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=2500)\n",
    "#bow = df2['cleaned_text'].apply(lambda x: vectorizer.fit_transform(x))\n",
    "#bow =  vectorizer.fit_transform(df2['cleaned_text'].head(20000))\n",
    "vectorizer.fit(df2['cleaned_text'].head(40000))\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "X = vectorizer.transform(df2['cleaned_text'].head(40000)).toarray()\n",
    "XX = vectorizer.transform(sentence).toarray()\n",
    "# summarize encoded vector\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "print(X)\n",
    "#to do: find the right size for the vocabulary and the number of reviews to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[1075])\n",
    "#create machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqydR9NB-rpW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df2.iloc[:, 4].head(40000).values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYVQeCaTAY0S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2500)\n",
      "(10000, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2500)\n",
      "(10000, 2500)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 2500)\n",
      "(7500, 2500)\n",
      "(7500,)\n",
      "(10000, 2500)\n",
      "(22500,)\n",
      "(22500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "#ohe = OneHotEncoder()\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "classifier = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(XX)\n",
    "print(y_pred)\n",
    "\n",
    "#print(confusion_matrix(y_test,y_pred))\n",
    "#print(classification_report(y_test,y_pred))\n",
    "#print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2500, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                40016     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                204       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 40,246\n",
      "Trainable params: 40,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-06e9953c11a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m                 batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "#Converting predictions to label\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "#Converting one hot encoded test label to label\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(pred,test)\n",
    "print('Accuracy is:', a*100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPl16+mKyE3UlupiU97bRDD",
   "collapsed_sections": [],
   "mount_file_id": "1nLa7wpFhG9jZYX0DMn_qjbr6M_CI4Hhm",
   "name": "detectPolarity.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
