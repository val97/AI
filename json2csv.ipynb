{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "#import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('names')\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from normalise import normalise\n",
    "import string\n",
    "import io\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get yelp dataset from kaggle\n",
    "!pip install -U -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!kaggle datasets download -d yelp-dataset/yelp-dataset\n",
    "!ls\n",
    "!unzip yelp-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json file to csv\n",
    "data = {'stars': [], 'text': []}\n",
    "n=0\n",
    "with open('./yelp_academic_dataset_review.json') as f:\n",
    "  for line in tqdm(f):\n",
    "      review = json.loads(line)\n",
    "      data['stars'].append(review['stars'])\n",
    "      data['text'].append(review['text'])\n",
    "      #data['useful'].append(review['useful'])\n",
    "      #data['funny'].append(review['funny'])\n",
    "      #data['cool'].append(review['cool'])\n",
    "      #data['business_id'].append(review['business_id'])\n",
    "      n = n+1\n",
    "\n",
    "  df = pd.DataFrame(data)\n",
    "  df.to_csv('yelp_academic_dataset_review.csv', sep='\\t')\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./drive/My Drive/yelp_academic_dataset_review.csv', sep='\\t',nrows=250000)\n",
    "del df[\"Unnamed: 0\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the interested column\n",
    "df2 = df[[\"stars\", \"text\"]]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers 0 or 6 stars\n",
    "df2 = df2.loc[(df2[\"stars\"] < 6) and (df2[\"stars\"] > 0)]\n",
    "df2.head()\n",
    "df2[\"stars\"] = df2[\"stars\"].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the dataset into good and bad reviews\n",
    "positiveReviews = df2.loc[df[\"stars\"] > 3]\n",
    "negativeReviews = df2.loc[df[\"stars\"] < 3]\n",
    "neutralReviews = df2.loc[df[\"stars\"] == 3]\n",
    "print(positiveReviews.shape)\n",
    "print(negativeReviews.shape)\n",
    "print(neutralReviews.shape)\n",
    "dp = positiveReviews[:50000]\n",
    "dn = negativeReviews[:50000]\n",
    "#create a subset of 100000 reviews of the yelp dataset\n",
    "df2 = dp.append(dn)\n",
    "df2 = shuffle(df2)\n",
    "df2.shape\n",
    "df2.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum and mean of word for each review\n",
    "print(df2['text'].str.len().max())\n",
    "print(df2['text'].str.len().mean())\n",
    "df2.loc[df2['text'].str.len()==5000, 'text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all not english reviews\n",
    "df2['language'] = df2['text'].apply( lambda x: langid.classify(x)[0])\n",
    "df2\n",
    "not_english_reviews = df2.loc[df2['language']!= 'en']\n",
    "not_english_reviews\n",
    "df2 = df2.loc[df2['language'] == 'en']\n",
    "not_english_reviews.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the subset created\n",
    "df2.to_csv('./drive/My Drive/subset_review.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text cleaning using nltk\n",
    "%%time\n",
    "#tokenization, stop word removal and punctuation removal using nltk\n",
    "#is slower than spacy\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "df2[\"text\"] = df2[\"text\"].str.lower()\n",
    "\n",
    "#df2['cleaned_text_nltk'] = df2['text'].head(10000).apply(lambda x: [stemmer.stem(t.lower()) for t in normalise(word_tokenize(x)) if ( (not t in stop_words ) and (t.isalpha()))])\n",
    "df2['cleaned_text_nltk'] = df2['text'].apply(lambda x: [stemmer.stem(t) for t in word_tokenize(x) if ( (not t in stop_words ) and (t.isalpha()))])\n",
    "\n",
    "#df2['cleaned_text'] = df2['cleaned_text'].apply( [t.lemma.lemmatize(word, pos = \"v\") for t in df2['cleaned_text']  ])\n",
    "#df2['cleaned_text'] = df2['cleaned_text'].apply( [t.lemma.lemmatize(word, pos = \"n\") for t in df2['cleaned_text']  ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai] *",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
